# NLP HW3 Report  

### Model Design and Concept  
這次作業主要透過實做一個 n gram 模型來實現克漏字預測的任務。其中，我使用的是 nltk 的工具來完成，在我的模型禮包含了 2 gram 和 3 gram 已達到更好的效果，並且為了解決零機率的問題，在計算機率時我使用了 laplace smoothing，也就是在分子和分母加上一個微小的值（在此分別為 1 和單詞總數）。對資料的前處理除了先把 training data 的空格填上正確的答案，主要是先把大小換成小寫，標點換成空格，在文字前後加上 padding，接著以空白為分隔做 tokenize。  
### Error Analysis and Discussion  
在觀察部份答案後，我認為對於 n gram 這樣的模型挑選大量並合適的語料庫是關鍵之一。然而，這樣仍無法解決同樣兩個字後面卻要接不同字的狀況，即使把模型設計的更複雜也終究只能減緩這樣的問題。再來，特殊名詞也會導致模型找不到答案，例如人名往往會超出 n gram 的能力範圍，只能看語料庫裡有沒有同名同姓的人剛好在做同樣的事情，不過這樣的狀況相對較少。  
### Problems and Solution  
以上提到的幾個問題中，首先是以模型的設計來讓選字不會被局現在特定選項中。我的作法是除了看前兩個字以外，同時也會考慮空格的後一個字，希望可以對某些情況有所幫助。另外，如果只使用 3 gram 作為模型，那我發現會有將近一半的問題無法在模型找到解答，因此我還加入了 2 gram model，以備上述情況發生時可以用 2 gram 來做彌補。  

除了模型的設計，其實再一開始瀏覽資料時我有發現訓練資料不乾淨的問題，在助教提供的訓練資料裡有許多文章中的空格前會有類似編號的多餘數字，因此我在資料前處理時有判斷這種狀況並將其篩掉。考慮到有些空格前本來就有數字，只有超過七成的空格前出現數字時我才會把該檔案做上述處理。  

最後的難點在於蒐集語料庫，其實這件事本身問題不大，麻煩的是一方面我不知道自己找到的語料庫內容是什麼（要閱讀並檢查內容需要花許多的時間），並且大量的語料庫會讓電腦執行得很慢，還會吃電腦的效能，因此後來我未能使用更多的語料庫來做實驗。  
### Influence with Different Data  
除了助教附的訓練資料，我曾使用 nltk 下載的語料庫和 wiki corpus，資料量是 wiki > nltk，依照 spec 的要求結果如下:  
| Corpus | Ta's data | nltk | wiki corpus |
| -------- | -------- | -------- | ------ |
| This is   | a       | the      | the    |
| He said   | i     | the      | that     |
| She said  | i     | i        | i        |  

看起來 I 和冠詞還是最常出現在這三組詞後面的單詞。在增加語料庫後，This is 後面都是接 the，而 He said 則因情況而異，She said 則比較常接 I，我想通常這會是一句話的開頭。  

---  
  
### Question 1  
1. Tokenizer and Build Window  
    我是將一篇文章的內容全轉成小寫再把標點去除後，直接用 nltk 裡的 RegexpTokenizer 把文字切成 token。其中的正規規則為 '\S+'，大寫的 S 代表非空白字元，這樣做是為了避免切出來的 token 裡包含空格。  
2. Answer Option  
    首先，由於使用迴圈遍歷四個選項時順序會維持在檔案裡看到的樣子，我使用 enumerate 關鍵字以確認目前遍歷到第幾個選項，並且建立了一個 dictionary AnsCode 以方便我把選項轉為目標字母。  
    ```python
    AnsCode = {0 : "A", 1 : "B", 2 : "C", 3 : "D"}
    ```
    查找選項的方法也不複雜，我是直接把四個選項輪流填進空格切成 token，再把它作為 ngram 輸出物件的 reference，用類似 dictionary 的方式查找，這樣做可以避免使用迴圈而浪費大量時間。  
    
    最後，我使用 3 gram 和 2 gram 配合以增加模型能力。我會先對同一組詞計算四種機率，分別是 p31 : word word _ , p32 : word _ word , p21 : word _ , p22 : _ word，底線代表挖空格的位置，word 代表相對空格前後的幾個文字，因此 p31 代表空格前兩個詞再加上用選項來填上空格的詞組所得的機率。我依照 p31\*p32, p31, p32, p21\*p22, p21, p22 這樣的順序來找出答案，也就是說如果可以在 3 gram 找到詞組就以 3 gram 為主，不行的話再往 2 gram 下去找。其中像是 p32 有考慮到空格後的一個詞，這是為了避免像是空格放在開頭時，如果只用前兩個詞判斷答案，那每次會得到的結果都會一樣，因此我決定綜合考慮 p31 和 p32 作為選擇方法。  
### Question 2  
我的模型大致如下(僅示意):
```python
class LanguageModel():
    def __init__(self, train_text, laplace=1):
        self.tokens = preprocess(train_text)
        self.laplace = laplace
        self.gram2 = self.model(2)
        self.gram3 = self.model(3)
    def model(self, n):
        # create a model with n gram
        # apply laplace smoothing with self.laplace
    def choose_opt():
        # choose best option according to p31, p32, p21, p22
    def get_ans():
        # enumerate each question, use self.choose_opt() to find the best answer
```  
模型的表現大概位於準確率 0.47 左右，從網路上查到的資料以及和同學討論後，我想以 ngram 解克漏字的任務五成的準確率左右差不多就是極限了，如果再修改資料的預處理、增加模型的規則和語料庫應該能再提升表現。